# -*- coding: utf-8 -*-
"""Text_Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q-yUQKvdl9rvBGu25WGD3XEh7xDikNhF
"""

from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer 
import math
import nltk
import contractions as contractions
import re
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""#1. Text Summarization Using TF-IDF



We used Extracted Summarization Method. In this keyphrases are selected which occurs in the document.

### Converting The Paragraph into Sentences
"""

def paragraphToSentence(paragraph):

    sentencesDict = {}
    sentencesList = sent_tokenize(paragraph)
    
    for i in range(len(sentencesList)):
        sentencesDict[i] = sentencesList[i].strip()
    
    return sentencesDict


def cleaning(file):
    documentsDict = {}
    for key in file:
        clean_S = contractions.fix(file[key])
        # clean_S=re.sub("([\-]{1,2})", " ",clean_S)
        # clean_s=re.sub("[\,]", " ",clean_S)
        # clean_S = re.sub("[\.]"," ",clean_S)
        clean_S = re.sub(r'[\!@#$%^&\*()\_+={}\:\;<>\?/\|\-/"\']*', "", clean_S)  # junk symbols
        # clean_S=re.sub("([\.\!]{2})", " ",clean_S)  #removing punctuations
        clean_S = re.sub('\s+', ' ', clean_S)
        clean_S = re.sub("[^a-zA-Z\s\n\.]", " ", clean_S)  # used to eliminate non-ascii and numeric symbols
        clean_S = re.sub(r"(.)\1{2}", "", clean_S)  # for elongated words
        clean_S = clean_S.strip()
        documentsDict[key] = clean_S

    return documentsDict

"""### Text Preprocessing
- Converting to Lowercase
- Tokenization & Punctuation Removal
- Removing Stopwords
- Lemmatization
"""

def preprocessing(sentenceDict):
    
    tokensDict = {}
    
    #Converting to Lowercase
    for sentenceNo in sentenceDict:
        sentence = sentenceDict[sentenceNo]
        tokensDict[sentenceNo] = sentence.lower()
    
    #Tokenization
    tokenizer = RegexpTokenizer(r'\w+')
    for sentenceNo in tokensDict:   
        sentence = tokensDict[sentenceNo]
        tokenSentenceList = tokenizer.tokenize(sentence)
        tokensDict[sentenceNo] = tokenSentenceList
            
    #Removing Stopwords
    stopwordsList = stopwords.words('english')
    
    for sentenceNo in tokensDict:
        sentenceTokenList = tokensDict[sentenceNo]

        for word in sentenceTokenList.copy():
            if word in stopwordsList:
                sentenceTokenList.remove(word)
            tokensDict[sentenceNo] = sentenceTokenList
            
    #Lemmatization        
    lemmatizer = WordNetLemmatizer() 
    for sentenceNo in tokensDict:
        tokenOutputList = []
        sentenceTokenList = tokensDict[sentenceNo]

        for word in sentenceTokenList:
            tokenOutputList.append(lemmatizer.lemmatize(word))
        tokensDict[sentenceNo] = tokenOutputList

            
    return tokensDict

"""### Creating Word Frequency Table"""

def frequenctTable(tokensDict):
    
    for sentenceNo in tokensDict:
        frequencyDict = {}
        sentenceTokenList = tokensDict[sentenceNo]
    
        for word in sentenceTokenList:
            if word not in frequencyDict:
                frequencyDict[word] = 1
            else:
                frequencyDict[word] += 1
        tokensDict[sentenceNo] = frequencyDict


    return tokensDict

"""### Calculating Term Frequency (TF)

"""

def termFrequency(frequencyTable):
    
    tfScoresDict = {}
    for sentenceNo in frequencyTable:
        frequencyDict = {}
        sentenceTokenDict = frequencyTable[sentenceNo]
    
        for term in sentenceTokenDict:
            frequencyDict[term] =  frequencyTable[sentenceNo][term] / len(sentenceTokenDict)

        tfScoresDict[sentenceNo] = frequencyDict

    return tfScoresDict

"""### Calculating Number of Sentences Containing a Word

"""

def sentenceContainingWord(frequencyTable):
    
    sentenceWordTable = {}
    
    for sentenceNo in frequencyTable:
        sentenceTokenDict = frequencyTable[sentenceNo]
        
        for word in sentenceTokenDict:
            if word not in sentenceWordTable:
                sentenceWordTable[word] = 1
            else:
                sentenceWordTable[word] += 1
    
    return sentenceWordTable

"""### Calculating Invese Document Frequency (IDF)"""

def inverseDf(frequencyTable, sentencesWordDict):
    
    inverseDfScores = {}
    for sentenceNo in frequencyTable:
        idfDict = {}
        sentenceTokenDict = frequencyTable[sentenceNo]

        for term in sentenceTokenDict:
            idfDict[term] = math.log10(len(frequencyTable)/ sentencesWordDict[term])
            
        inverseDfScores[sentenceNo] = idfDict
        
    return inverseDfScores

"""### Calculating TF-IDF"""

def calculateTfIdf(tfScores, inverseDfScores):
    
    tfidfScores = {}
        
    for sentenceNo in tfScores:
        tfidfDict = {}
        
        tfDict = tfScores[sentenceNo]
        idfDict = inverseDfScores[sentenceNo]
        
        for tf, idf in zip(tfDict, idfDict):
            tfidfDict[tf] = tfDict[tf] * idfDict[idf]
            
        tfidfScores[sentenceNo] = tfidfDict
        
    return tfidfScores

"""### Scoring Each Sentence"""

def sentenceScoring(tfIdfScores):
    scoresList = []
    
    for sentenceNo in tfIdfScores:
        tfIdfScoresDict = tfIdfScores[sentenceNo]
        scoresList.append(sum(list(tfIdfScoresDict.values())) / len(tfIdfScoresDict))
    
    return scoresList

"""### Generating Summary"""

def generatingSummary(sentenceDict, sentenceScores, threshold):
    
    summarySentences = []
    for scores in sentenceScores:
        if scores > threshold:
            summarySentences.append(sentenceScores.index(scores))
            
    summary = ''
    if summarySentences:
        for sentenceNo in summarySentences:
            summary += sentenceDict[sentenceNo] + ' '
            
    return summary.strip()

def getSummary(paragraph):
    sentenceDict = paragraphToSentence(paragraph)
    documentsDict = cleaning(sentenceDict)
    tokensDict = preprocessing(sentenceDict)
    frequencyTable = frequenctTable(tokensDict)
    tfScores = termFrequency(frequencyTable)
    sentencesWordDict = sentenceContainingWord(frequencyTable)
    idfScores = inverseDf(frequencyTable, sentencesWordDict)
    tfIdfScores = calculateTfIdf(tfScores, idfScores)
    sentenceScores = sentenceScoring(tfIdfScores)
    threshold = sum(sentenceScores) / len(sentenceDict)
    summary = generatingSummary(documentsDict, sentenceScores, threshold)
    return summary


